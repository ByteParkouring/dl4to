{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63908be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f922b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import importlib.util\n",
    "import warnings\n",
    "from scipy.sparse.linalg import factorized, use_solver, spsolve\n",
    "from scipy.sparse import csc_matrix\n",
    "from typing import Callable\n",
    "\n",
    "use_solver(assumeSortedIndices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f894fe4-db6c-4c86-8efa-2a886074ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d182e-f662-4b85-983b-0e62dfcdcf69",
   "metadata": {},
   "source": [
    "# Linear solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a5b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AutogradLinearSolver(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, θ, A_op, b, solver, A_mat, factorize=True):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a tensor containing the input and return\n",
    "        a tensor containing the output. `ctx` is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the `ctx.save_for_backward` method.\n",
    "\n",
    "        Returns\n",
    "        torch.Tensor\n",
    "        \"\"\"\n",
    "        np_b = b.cpu().numpy()\n",
    "\n",
    "        if factorize:\n",
    "            solver = factorized(A_mat)\n",
    "            x = solver(np_b)\n",
    "        else:\n",
    "            x = solver(A_mat, np_b)\n",
    "\n",
    "        x = torch.from_numpy(x.astype(np_b.dtype))\n",
    "        ctx.save_for_backward(θ, x, b)\n",
    "        ctx.intermediate = (A_mat, solver, A_op, factorize)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        (torch.Tensor, None, None, None, None, None)\n",
    "        \"\"\"\n",
    "        torch.set_grad_enabled(True)\n",
    "        θ, x, b = ctx.saved_tensors\n",
    "        A_mat, solver, A_op, factorize = ctx.intermediate\n",
    "        θ = θ.clone().detach()\n",
    "        θ.requires_grad_(True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            flat_np_grad_output = grad_output.flatten().cpu().numpy()\n",
    "\n",
    "            if factorize:\n",
    "                y = solver(flat_np_grad_output)\n",
    "            else:\n",
    "                y = solver(A_mat, flat_np_grad_output)\n",
    "\n",
    "            y = torch.from_numpy(y).clone().requires_grad_(False)\n",
    "            x = x.clone().requires_grad_(False)\n",
    "\n",
    "        expr = torch.sum(y * (b - A_op(x, θ).flatten()))\n",
    "        grad_input = torch.autograd.grad(expr, θ)\n",
    "        return grad_input[0], None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LinearSolver():\n",
    "    \"\"\"\n",
    "    A parent class for linear solvers that are used to solve the linear system that solves the PDE.\n",
    "    We compute the gradients via `torch.autograd` and with the adjoint method in the backwards pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 factorize:bool=True # Whether the system matrix should be factorized.\n",
    "                ):\n",
    "        self.autograd_linear_solver = AutogradLinearSolver.apply\n",
    "        self.factorize = factorize\n",
    "\n",
    "\n",
    "    def _solver(self):\n",
    "        raise NotImplementedError(\"Solver must be overridden.\")\n",
    "\n",
    "\n",
    "    def __call__(self, \n",
    "                 θ:torch.Tensor, # The density for which the PDE is solved.\n",
    "                 A_op:Callable[[torch.Tensor, torch.Tensor], torch.Tensor], # A function that takes `u` and `θ` as input and outputs the right hand side of the PDE. In other words, this is an operator representing the system matrix.\n",
    "                 b:torch.Tensor, # A flattened version of the right side of the PDE.\n",
    "                 A_mat:csc_matrix # The system matrix in sparse format.\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Solves the PDE for the density `θ`. Returns the solution as a `torch.Tensor` object.\n",
    "        \"\"\"\n",
    "        x = self.autograd_linear_solver(θ, A_op, b, self._solver(), A_mat, self.factorize)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb517e1-4d3c-4c71-b586-3fd48665f41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"LinearSolver.__call__\" class=\"doc_header\"><code>LinearSolver.__call__</code><a href=\"__main__.py#L18\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>LinearSolver.__call__</code>(**`θ`**:`Tensor`, **`A_op`**:`Callable`\\[`Tensor`, `Tensor`, `Tensor`\\], **`b`**:`Tensor`, **`A_mat`**:`csc_matrix`)\n",
       "\n",
       "Solves the PDE for the density `θ`. Returns the solution as a `torch.Tensor` object.\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`θ`**|`Tensor`||The density for which the PDE is solved.|\n",
       "|**`A_op`**|`typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`||A function that takes `u` and `theta` as input and outputs the right hand side of the PDE. In other words, this is an operator representing the system matrix.|\n",
       "|**`b`**|`Tensor`||A flattened version of the right side of the PDE.|\n",
       "|**`A_mat`**|`csc_matrix`||The system matrix in sparse format.|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LinearSolver.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1795a04-f2ee-43c0-9e51-f6ea72bcbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SparseLinearSolver(LinearSolver):\n",
    "    \"\"\"\n",
    "    A sparse linear solver implementation based on the `scipy.sparse.linalg.solve()` method that is used to solve the PDE of linear elasticity.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 use_umfpack:bool=True, # Whether to use umfpack. If false, then the LU solver from `scipy.sparse` is used, which is usually slower.\n",
    "                 factorize:bool=False # Whether the system matrix should be factorized.\n",
    "                ):\n",
    "        if use_umfpack or factorize:\n",
    "            if importlib.util.find_spec('scikits') is None:\n",
    "                warnings.warn(\"The package scikits.umfpack is not installed.Therefore, the LU solver from scipy.sparse is used, which is usually slower.\")\n",
    "\n",
    "        self.use_umfpack = use_umfpack\n",
    "        super().__init__(factorize)\n",
    "\n",
    "\n",
    "    def _solver(self):\n",
    "        return lambda A, b: spsolve(A, b, use_umfpack=self.use_umfpack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b24bc-f57e-4086-8637-86c9e407de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ed62e-50ac-4471-bb07-e49eb8d6e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def get_operator_and_b():\n",
    "    n = 4\n",
    "    θ = torch.rand(1,n,n,n, requires_grad=True)\n",
    "    θ_triple = torch.cat([θ, θ, θ])\n",
    "\n",
    "    def A_op(x, θ):\n",
    "        x = x.view(3,n,n,n)\n",
    "        θ_triple = torch.cat([θ, θ, θ])\n",
    "        return θ_triple*x\n",
    "\n",
    "    A_mat = np.diag(θ_triple.flatten().detach().numpy()).astype(np.float64)\n",
    "\n",
    "    b = torch.ones(3,n,n,n)\n",
    "\n",
    "    return A_op, A_mat, b, θ, θ_triple, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f593b-007d-4e1b-9f6b-93281c603f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.87 ms, sys: 8.62 ms, total: 12.5 ms\n",
      "Wall time: 30 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erzmann/anaconda3/envs/dl4to/lib/python3.9/site-packages/scipy/sparse/linalg/dsolve/linsolve.py:144: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  warn('spsolve requires A be CSC or CSR matrix format',\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#hide\n",
    "\n",
    "def test_that_we_can_solve_a_system():\n",
    "    A_op, A_mat, b, θ, θ_triple, n = get_operator_and_b()\n",
    "\n",
    "    solver = SparseLinearSolver()\n",
    "    x = solver(θ=θ, A_op=A_op, b=b.flatten(), A_mat=A_mat)\n",
    "    x = x.view(3, n, n, n)\n",
    "    assert torch.allclose(A_op(x, θ), b)\n",
    "    assert torch.allclose(x, 1/θ_triple)\n",
    "\n",
    "\n",
    "test_that_we_can_solve_a_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b59b5-5bb4-4e80-9624-b65019466454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.5 ms, sys: 5 ms, total: 69.5 ms\n",
      "Wall time: 67.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#hide\n",
    "#slow\n",
    "def test_that_we_can_differentiate_solution(verbose):\n",
    "    A_op, A_mat, b, θ, θ_triple, n = get_operator_and_b()\n",
    "    solver = SparseLinearSolver()\n",
    "\n",
    "    optimizer = torch.optim.Adam([θ],lr=1e-1)\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(20):\n",
    "        θ_triple = torch.cat([θ, θ, θ])\n",
    "        A_mat = np.diag(θ_triple.flatten().detach().numpy()).astype(np.float64)\n",
    "        x = solver(θ=θ, A_op=A_op, b=b.flatten(), A_mat=A_mat)\n",
    "        x = x.view(3, n, n, n)\n",
    "        loss = torch.nn.functional.mse_loss(x, torch.ones_like(x))\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if verbose:\n",
    "        plt.plot(losses)\n",
    "        plt.show()\n",
    "        print(losses[-1])\n",
    "\n",
    "    assert losses[-1] < .2\n",
    "\n",
    "\n",
    "test_that_we_can_differentiate_solution(verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
