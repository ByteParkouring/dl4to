{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96fad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "import torch\n",
    "import torch.autograd.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ea231-0266-4fee-bf9f-521dda940111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9b467",
   "metadata": {},
   "source": [
    "# FDM derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15b2c7-06e1-46cd-b4c1-de4bc0bb8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FDMDerivatives():\n",
    "    @staticmethod\n",
    "    def du_dx_central(u, h):\n",
    "        du = torch.zeros_like(u)\n",
    "        du[:, 1:-1, :,:] = (u[:,  2:, :,:] - u[:, 0:-2, :,:]) / (2 * h[0])\n",
    "        du[:,  0  , :,:] = (u[:,  1 , :,:] - u[:,  0  , :,:]) / h[0]\n",
    "        du[:, -1  , :,:] = (u[:, -1 , :,:] - u[:, -2  , :,:]) / h[0]\n",
    "        return du\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dy_central(u, h):\n",
    "        du = torch.zeros_like(u)\n",
    "        du[:,:, 1:-1, :] = (u[:,:,  2:, :] - u[:,:, 0:-2, :]) / (2 * h[1])\n",
    "        du[:,:,  0  , :] = (u[:,:,  1 , :] - u[:,:,  0  , :]) / h[1]\n",
    "        du[:,:, -1  , :] = (u[:,:, -1 , :] - u[:,:, -2  , :]) / h[1]\n",
    "        return du\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dz_central(u, h):\n",
    "        du = torch.zeros_like(u)\n",
    "        du[:,:,:, 1:-1] = (u[:,:,:,  2:] - u[:,:,:, 0:-2]) / (2 * h[2])\n",
    "        du[:,:,:,  0  ] = (u[:,:,:,  1 ] - u[:,:,:,  0  ]) / h[2]\n",
    "        du[:,:,:, -1  ] = (u[:,:,:, -1 ] - u[:,:,:, -2  ]) / h[2]\n",
    "        return du\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dx_forward(u, h):\n",
    "        du = torch.zeros_like(u)\n",
    "        du[:, 0:-1,:,:] = (u[:,  1:,:,:] - u[:, 0:-1,:,:]) / h[0]\n",
    "        du[:, -1  ,:,:] = (u[:, -1 ,:,:] - u[:, -2  ,:,:]) / h[0]\n",
    "        return du\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dy_forward(u, h):\n",
    "        du = torch.zeros_like(u)\n",
    "        du[:,:, 0:-1,:] = (u[:,:,  1:,:] - u[:,:, 0:-1,:]) / h[1]\n",
    "        du[:,:, -1  ,:] = (u[:,:, -1 ,:] - u[:,:, -2  ,:]) / h[1]\n",
    "        return du\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dz_forward(u, h):\n",
    "        du = torch.zeros_like(u)\n",
    "        du[:,:,:, 0:-1] = (u[:,:,:,  1:] - u[:,:,:, 0:-1]) / h[2]\n",
    "        du[:,:,:, -1  ] = (u[:,:,:, -1 ] - u[:,:,:, -2  ]) / h[2]\n",
    "        return du\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dx(u, h, use_forward_differences=True):\n",
    "        assert len(u.shape) == 4\n",
    "        assert u.shape[1] > 2\n",
    "        if use_forward_differences:\n",
    "            return FDMDerivatives.du_dx_forward(u, h)\n",
    "        return FDMDerivatives.du_dx_central(u, h)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dy(u, h, use_forward_differences=True):\n",
    "        assert len(u.shape) == 4\n",
    "        assert u.shape[2] > 2\n",
    "        if use_forward_differences:\n",
    "            return FDMDerivatives.du_dy_forward(u, h)\n",
    "        return FDMDerivatives.du_dy_central(u, h)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dz(u, h, use_forward_differences=True):\n",
    "        assert len(u.shape) == 4\n",
    "        assert u.shape[3] > 2\n",
    "        if use_forward_differences:\n",
    "            return FDMDerivatives.du_dz_forward(u, h)\n",
    "        return FDMDerivatives.du_dz_central(u, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76c158",
   "metadata": {},
   "source": [
    "### Hardcoded analytical adjoints of numerical derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FDMAdjointDerivatives():\n",
    "    @staticmethod\n",
    "    def du_dx_adj_for_a_sufficiently_large_number_of_voxels(ε, h):\n",
    "        u = torch.zeros_like(ε)\n",
    "        u[:,   0,:,:] = -(2 * ε[:,   0,:,:] + ε[:,   1,:,:]) / (2 * h[0])\n",
    "        u[:,   1,:,:] =  (2 * ε[:,   0,:,:] - ε[:,   2,:,:]) / (2 * h[0])\n",
    "        u[:,2:-2,:,:] =  (    ε[:,1:-3,:,:] - ε[:,3:-1,:,:]) / (2 * h[0])\n",
    "        u[:,  -2,:,:] = -(2 * ε[:,  -1,:,:] - ε[:,  -3,:,:]) / (2 * h[0])\n",
    "        u[:,  -1,:,:] =  (2 * ε[:,  -1,:,:] + ε[:,  -2,:,:]) / (2 * h[0])\n",
    "        return u\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dy_adj_for_a_sufficiently_large_number_of_voxels(ε, h):\n",
    "        u = torch.zeros_like(ε)\n",
    "        u[:,:,   0,:] = -(2 * ε[:,:,   0,:] + ε[:,:,   1,:]) / (2 * h[1])\n",
    "        u[:,:,   1,:] =  (2 * ε[:,:,   0,:] - ε[:,:,   2,:]) / (2 * h[1])\n",
    "        u[:,:,2:-2,:] =  (    ε[:,:,1:-3,:] - ε[:,:,3:-1,:]) / (2 * h[1])\n",
    "        u[:,:,  -2,:] = -(2 * ε[:,:,  -1,:] - ε[:,:,  -3,:]) / (2 * h[1])\n",
    "        u[:,:,  -1,:] =  (2 * ε[:,:,  -1,:] + ε[:,:,  -2,:]) / (2 * h[1])\n",
    "        return u\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dz_adj_for_a_sufficiently_large_number_of_voxels(ε, h):\n",
    "        u = torch.zeros_like(ε)\n",
    "        u[:,:,:,   0] = -(2 * ε[:,:,:,   0] + ε[:,:,:,   1]) / (2 * h[2])\n",
    "        u[:,:,:,   1] =  (2 * ε[:,:,:,   0] - ε[:,:,:,   2]) / (2 * h[2])\n",
    "        u[:,:,:,2:-2] =  (    ε[:,:,:,1:-3] - ε[:,:,:,3:-1]) / (2 * h[2])\n",
    "        u[:,:,:,  -2] = -(2 * ε[:,:,:,  -1] - ε[:,:,:,  -3]) / (2 * h[2])\n",
    "        u[:,:,:,  -1] =  (2 * ε[:,:,:,  -1] + ε[:,:,:,  -2]) / (2 * h[2])\n",
    "        return u\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dx_adj_for_a_sufficiently_small_number_of_voxels(ε, h):\n",
    "        u = torch.zeros_like(ε)\n",
    "\n",
    "        if u.shape[1] == 2:\n",
    "            u[:, 0,:,:] = -(ε[:, 0,:,:] +  ε[:, 1,:,:]) / h[0]\n",
    "            u[:, 1,:,:] = - u[:, 0,:,:]\n",
    "\n",
    "        if u.shape[1] == 3:\n",
    "            u[:,0,:,:] = -(2 * ε[:,0,:,:] + ε[:,1,:,:]) / (2 * h[0])\n",
    "            u[:,1,:,:] =  (    ε[:,0,:,:] - ε[:,2,:,:]) /  h[0]\n",
    "            u[:,2,:,:] =  (2 * ε[:,2,:,:] + ε[:,1,:,:]) / (2 * h[0])\n",
    "\n",
    "        return u\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dy_adj_for_a_sufficiently_small_number_of_voxels(ε, h):\n",
    "        u = torch.zeros_like(ε)\n",
    "\n",
    "        if u.shape[2] == 2:\n",
    "            u[:,:, 0,:] = -(ε[:,:, 0,:] +  ε[:,:, 1,:]) / h[1]\n",
    "            u[:,:, 1,:] = - u[:,:, 0,:]\n",
    "\n",
    "        if u.shape[2] == 3:\n",
    "            u[:,:,0,:] = -(2 * ε[:,:,0,:] + ε[:,:,1,:]) / (2 * h[1])\n",
    "            u[:,:,1,:] =  (    ε[:,:,0,:] - ε[:,:,2,:]) /  h[1]\n",
    "            u[:,:,2,:] =  (2 * ε[:,:,2,:] + ε[:,:,1,:]) / (2 * h[1])\n",
    "\n",
    "        return u\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dz_adj_for_a_sufficiently_small_number_of_voxels(ε, h):\n",
    "        u = torch.zeros_like(ε)\n",
    "\n",
    "        if u.shape[3] == 2:\n",
    "            u[:,:,:, 0] = -(ε[:,:,:, 0] +  ε[:,:,:, 1]) / h[2]\n",
    "            u[:,:,:, 1] = - u[:,:,:, 0]\n",
    "\n",
    "        if u.shape[3] == 3:\n",
    "            u[:,:,:,0] = -(2 * ε[:,:,:,0] + ε[:,:,:,1]) / (2 * h[2])\n",
    "            u[:,:,:,1] =  (    ε[:,:,:,0] - ε[:,:,:,2]) /  h[2]\n",
    "            u[:,:,:,2] =  (2 * ε[:,:,:,2] + ε[:,:,:,1]) / (2 * h[2])\n",
    "\n",
    "        return u\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dx_adj_forward(ε, h):\n",
    "        u = torch.zeros_like(ε)\n",
    "        u[:,   0   ,:,:] =  (                 - ε[:,      0,:,:]) / h[0]\n",
    "        u[:,   1:-2,:,:] =  (ε[:,   0:-3,:,:] - ε[:,   1:-2,:,:]) / h[0]\n",
    "        u[:,     -2,:,:] =  (ε[:,  -3,:,:] - ε[:,  -2,:,:] - ε[:,  -1,:,:]) / h[0]\n",
    "        u[:,     -1,:,:] =  (ε[:,     -2,:,:] + ε[:,     -1,:,:]) / h[0]\n",
    "        return u\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dy_adj_forward(ε, h):\n",
    "        u = torch.zeros_like(ε)\n",
    "        u[:,:,   0   ,:] =  (                 - ε[:,:,      0,:]) / h[1]\n",
    "        u[:,:,   1:-2,:] =  (ε[:,:,   0:-3,:] - ε[:,:,   1:-2,:]) / h[1]\n",
    "        u[:,:,     -2,:] =  (ε[:,:,  -3,:] - ε[:,:,  -2,:] - ε[:,:,  -1,:]) / h[1]\n",
    "        u[:,:,     -1,:] =  (ε[:,:,     -2,:] + ε[:,:,     -1,:]) / h[1]\n",
    "        return u\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dz_adj_forward(ε, h):\n",
    "        u = torch.zeros_like(ε)\n",
    "        u[:,:,:,   0   ] =  (                 - ε[:,:,:,      0]) / h[2]\n",
    "        u[:,:,:,   1:-2] =  (ε[:,:,:,   0:-3] - ε[:,:,:,   1:-2]) / h[2]\n",
    "        u[:,:,:,     -2] =  (ε[:,:,:,  -3] - ε[:,:,:,  -2] - ε[:,:,:,  -1]) / h[2]\n",
    "        u[:,:,:,     -1] =  (ε[:,:,:,     -2] + ε[:,:,:,     -1]) / h[2]\n",
    "        return u\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dx_adj(ε, h, use_forward_differences=True):\n",
    "        assert len(ε.shape) == 4\n",
    "        if use_forward_differences:\n",
    "            return FDMAdjointDerivatives.du_dx_adj_forward(ε, h)\n",
    "\n",
    "\n",
    "        if ε.shape[1] > 3:\n",
    "            return FDMAdjointDerivatives.du_dx_adj_for_a_sufficiently_large_number_of_voxels(ε, h)\n",
    "        return FDMAdjointDerivatives.du_dx_adj_for_a_sufficiently_small_number_of_voxels(ε, h)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dy_adj(ε, h, use_forward_differences=True):\n",
    "        assert len(ε.shape) == 4\n",
    "        if use_forward_differences:\n",
    "            return FDMAdjointDerivatives.du_dy_adj_forward(ε, h)\n",
    "\n",
    "\n",
    "        if ε.shape[2] > 3:\n",
    "            return FDMAdjointDerivatives.du_dy_adj_for_a_sufficiently_large_number_of_voxels(ε, h)\n",
    "        return FDMAdjointDerivatives.du_dy_adj_for_a_sufficiently_small_number_of_voxels(ε, h)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def du_dz_adj(ε, h, use_forward_differences=True):\n",
    "        assert len(ε.shape) == 4\n",
    "        if use_forward_differences:\n",
    "            return FDMAdjointDerivatives.du_dz_adj_forward(ε, h)\n",
    "\n",
    "\n",
    "        if ε.shape[3] > 3:\n",
    "            return FDMAdjointDerivatives.du_dz_adj_for_a_sufficiently_large_number_of_voxels(ε, h)\n",
    "        return FDMAdjointDerivatives.du_dz_adj_for_a_sufficiently_small_number_of_voxels(ε, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c173fcb",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hypothesis.strategies as st\n",
    "import hypothesis.extra.numpy as npst\n",
    "from hypothesis import given, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7612688",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3 * [1e-3]\n",
    "atol = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327553dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_u_ε_shape = st.tuples(st.integers(min_value=6, max_value=6),\n",
    "                         st.integers(min_value=3, max_value=32),\n",
    "                         st.integers(min_value=3, max_value=32),\n",
    "                         st.integers(min_value=3, max_value=32))\n",
    "st_u_ε = npst.arrays(float, shape=st_u_ε_shape, elements=st.floats(-1e3, 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falsifying example: test_that_adjoint_derivative_really_is_the_adjoint(\n",
      "    u_ε=array([[[[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]]],\n",
      "    \n",
      "    \n",
      "           [[[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]]],\n",
      "    \n",
      "    \n",
      "           [[[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]]],\n",
      "    \n",
      "    \n",
      "           [[[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]]],\n",
      "    \n",
      "    \n",
      "           [[[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]]],\n",
      "    \n",
      "    \n",
      "           [[[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]],\n",
      "    \n",
      "            [[0., 0., 0.],\n",
      "             [0., 0., 0.],\n",
      "             [0., 0., 0.]]]]),\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:2\u001b[0m, in \u001b[0;36mtest_that_adjoint_derivative_really_is_the_adjoint\u001b[0;34m()\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:10\u001b[0m, in \u001b[0;36mtest_that_adjoint_derivative_really_is_the_adjoint\u001b[0;34m(u_ε)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@given(u_ε=st_u_ε)\n",
    "@settings(max_examples=10, deadline=None)\n",
    "def test_that_adjoint_derivative_really_is_the_adjoint(u_ε):\n",
    "    u = torch.tensor(u_ε[:3])\n",
    "    ε = torch.tensor(u_ε[3:])\n",
    "    forwDif = False\n",
    "\n",
    "    assert torch.allclose(torch.dot(u.flatten(), FDMAdjointDerivatives.du_dx_adj(ε, h, forwDif).flatten()), torch.dot(ε.flatten(), FDMDerivatives.du_dx(u, h, forwDif).flatten()), atol=atol)\n",
    "    assert torch.allclose(torch.dot(u.flatten(), FDMAdjointDerivatives.du_dy_adj(ε, h, forwDif).flatten()), torch.dot(ε.flatten(), FDMDerivatives.du_dy(u, h, forwDif).flatten()), atol=atol)\n",
    "    assert torch.allclose(torch.dot(u.flatten(), FDMAdjointDerivatives.du_dz_adj(ε, h, forwDif).flatten()), torch.dot(ε.flatten(), FDMDerivatives.du_dz(u, h, forwDif).flatten()), atol=atol)\n",
    "\n",
    "test_that_adjoint_derivative_really_is_the_adjoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.8 s, sys: 374 ms, total: 44.2 s\n",
      "Wall time: 492 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@given(u_ε=st_u_ε)\n",
    "@settings(max_examples=10, deadline=None)\n",
    "def test_that_adjoint_derivative_really_is_the_adjoint_for_forward_diffs(u_ε):\n",
    "    u = torch.tensor(u_ε[:3])\n",
    "    ε = torch.tensor(u_ε[3:])\n",
    "    forwDif = True\n",
    "\n",
    "    assert torch.allclose(torch.dot(u.flatten(), FDMAdjointDerivatives.du_dx_adj(ε, h, forwDif).flatten()), torch.dot(ε.flatten(), FDMDerivatives.du_dx(u, h, forwDif).flatten()), atol=atol)\n",
    "    assert torch.allclose(torch.dot(u.flatten(), FDMAdjointDerivatives.du_dy_adj(ε, h, forwDif).flatten()), torch.dot(ε.flatten(), FDMDerivatives.du_dy(u, h, forwDif).flatten()), atol=atol)\n",
    "    assert torch.allclose(torch.dot(u.flatten(), FDMAdjointDerivatives.du_dz_adj(ε, h, forwDif).flatten()), torch.dot(ε.flatten(), FDMDerivatives.du_dz(u, h, forwDif).flatten()), atol=atol)\n",
    "\n",
    "\n",
    "test_that_adjoint_derivative_really_is_the_adjoint_for_forward_diffs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2642629",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_x_shape = st.tuples(st.integers(min_value=1, max_value=1))\n",
    "st_x = npst.arrays(float, shape=st_x_shape, elements=st.floats(-1e2, 1e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23b67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falsifying example: test_that_fdm_derivative_and_torch_derivative_coincide_for_sin(\n",
      "    x=array([0.]),\n",
      ")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:2\u001b[0m, in \u001b[0;36mtest_that_fdm_derivative_and_torch_derivative_coincide_for_sin\u001b[0;34m()\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:13\u001b[0m, in \u001b[0;36mtest_that_fdm_derivative_and_torch_derivative_coincide_for_sin\u001b[0;34m(x)\u001b[0m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mFDMDerivatives.du_dx\u001b[0;34m(u, h, use_forward_differences)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdu_dx\u001b[39m(u, h, use_forward_differences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(u\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m u\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_forward_differences:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m FDMDerivatives\u001b[38;5;241m.\u001b[39mdu_dx_forward(u, h)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@given(x=st_x)\n",
    "@settings(max_examples=10, deadline=None)\n",
    "def test_that_fdm_derivative_and_torch_derivative_coincide_for_sin(x):\n",
    "    x = torch.tensor(np.array([x]), requires_grad=True)\n",
    "    y = torch.sin(x)\n",
    "    y.backward()\n",
    "    dy_dx_torch = x.grad\n",
    "\n",
    "    h = 1e-4\n",
    "    x_ = torch.tensor([x-h, x+h]).view(1, 2, 1, 1)\n",
    "    x_.requires_grad_(True)\n",
    "    y_ = torch.sin(x_)\n",
    "    dy_dx_fdm = FDMDerivatives.du_dx(y_, 3 * [2*h])[0, 0]\n",
    "    assert torch.allclose(dy_dx_torch, dy_dx_fdm, atol=atol)\n",
    "\n",
    "    x_ = torch.tensor([x-h, x+h]).view(1, 1, 2, 1)\n",
    "    x_.requires_grad_(True)\n",
    "    y_ = torch.sin(x_)\n",
    "    dy_dx_fdm = FDMDerivatives.du_dy(y_, 3 * [2*h])[0, 0]\n",
    "    assert torch.allclose(dy_dx_torch, dy_dx_fdm, atol=atol)\n",
    "\n",
    "    x_ = torch.tensor([x-h, x+h]).view(1, 1, 1, 2)\n",
    "    x_.requires_grad_(True)\n",
    "    y_ = torch.sin(x_)\n",
    "    dy_dx_fdm = FDMDerivatives.du_dz(y_, 3 * [2*h])[0, 0]\n",
    "    assert torch.allclose(dy_dx_torch, dy_dx_fdm, atol=atol)\n",
    "\n",
    "\n",
    "test_that_fdm_derivative_and_torch_derivative_coincide_for_sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97597b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falsifying example: test_that_fdm_derivative_and_torch_derivative_coincide_for_exp(\n",
      "    x=array([0.]),\n",
      ")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:2\u001b[0m, in \u001b[0;36mtest_that_fdm_derivative_and_torch_derivative_coincide_for_exp\u001b[0;34m()\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:13\u001b[0m, in \u001b[0;36mtest_that_fdm_derivative_and_torch_derivative_coincide_for_exp\u001b[0;34m(x)\u001b[0m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mFDMDerivatives.du_dx\u001b[0;34m(u, h, use_forward_differences)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdu_dx\u001b[39m(u, h, use_forward_differences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(u\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m u\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_forward_differences:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m FDMDerivatives\u001b[38;5;241m.\u001b[39mdu_dx_forward(u, h)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@given(x=st_x)\n",
    "@settings(max_examples=10, deadline=None)\n",
    "def test_that_fdm_derivative_and_torch_derivative_coincide_for_exp(x):\n",
    "    x = torch.tensor(np.array([x]), requires_grad=True)\n",
    "    y = torch.exp(x)\n",
    "    y.backward()\n",
    "    dy_dx_torch = x.grad\n",
    "\n",
    "    h = 1e-4\n",
    "    x_ = torch.tensor([x-h, x+h]).view(1, 2, 1, 1)\n",
    "    x_.requires_grad_(True)\n",
    "    y_ = torch.exp(x_)\n",
    "    dy_dx_fdm = FDMDerivatives.du_dx(y_, 3 * [2*h])[0, 0]\n",
    "    assert torch.allclose(dy_dx_torch, dy_dx_fdm, atol=atol)\n",
    "\n",
    "    x_ = torch.tensor([x-h, x+h]).view(1, 1, 2, 1)\n",
    "    x_.requires_grad_(True)\n",
    "    y_ = torch.exp(x_)\n",
    "    dy_dx_fdm = FDMDerivatives.du_dy(y_, 3 * [2*h])[0, 0]\n",
    "    assert torch.allclose(dy_dx_torch, dy_dx_fdm, atol=atol)\n",
    "\n",
    "    x_ = torch.tensor([x-h, x+h]).view(1, 1, 1, 2)\n",
    "    x_.requires_grad_(True)\n",
    "    y_ = torch.exp(x_)\n",
    "    dy_dx_fdm = FDMDerivatives.du_dz(y_, 3 * [2*h])[0, 0]\n",
    "    assert torch.allclose(dy_dx_torch, dy_dx_fdm, atol=atol)\n",
    "\n",
    "\n",
    "test_that_fdm_derivative_and_torch_derivative_coincide_for_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f42ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
