{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792fb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f42fcf-4b51-4d17-be3a-2c8443c79356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5bd21",
   "metadata": {},
   "source": [
    "# Deep Image Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ccb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DeepImagePrior(nn.Module):\n",
    "    \"\"\"\n",
    "    The deep image prior (DIP) [1] is a type of convolutional neural network used to enhance a given image with no prior training data other than the image itself.\n",
    "    A neural network is randomly initialized and used as prior to solve inverse problems such as noise reduction, super-resolution, and inpainting.\n",
    "    Image statistics are captured by the structure of a convolutional image generator rather than by any previously learned capabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 shape:list, # A list containing three entries that define the number of voxels in each direction.\n",
    "                 n_channels:int=1, # The number of input channels.\n",
    "                 n_inital_channels:int=4 #T he number of channels after the first encoding block. The model has a total 4 encoding and 4 decoding blocks, and the number of channels is doubled in each encoding step.\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        self.encoder_layers.append(nn.Conv3d(n_channels,    1*n_inital_channels, 3, stride=2, padding=1))\n",
    "        self.encoder_layers.append(nn.Conv3d(1*n_inital_channels, 2*n_inital_channels, 3, stride=2, padding=1))\n",
    "        self.encoder_layers.append(nn.Conv3d(2*n_inital_channels, 4*n_inital_channels, 3, stride=2, padding=1))\n",
    "        self.encoder_layers.append(nn.Conv3d(4*n_inital_channels, 8*n_inital_channels, 3, stride=2, padding=1))\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        self.decoder_layers.append(nn.Conv3d(8*n_inital_channels, 4*n_inital_channels, 3, stride=1, padding=1))\n",
    "        self.decoder_layers.append(nn.Conv3d(4*n_inital_channels, 2*n_inital_channels, 3, stride=1, padding=1))\n",
    "        self.decoder_layers.append(nn.Conv3d(2*n_inital_channels, 1*n_inital_channels, 3, stride=1, padding=1))\n",
    "        self.decoder_layers.append(nn.Conv3d(1*n_inital_channels,    n_channels, 3, stride=1, padding=1))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.z = torch.randn(n_channels, *shape, requires_grad=False)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        The forward pass of the DIP with a fixed random noise input. Returns a `torch.Tensor` object.\n",
    "        \"\"\"\n",
    "        encoder_activations = [self.z.view(1, *self.z.shape)]\n",
    "\n",
    "        for encoder_layer in self.encoder_layers[:-1]:\n",
    "            activation = encoder_activations[-1]\n",
    "            activation = nn.functional.layer_norm(activation, encoder_activations[-1].shape[1:])\n",
    "            activation = encoder_layer(activation)\n",
    "            activation = self.relu(activation)\n",
    "            encoder_activations.append(activation)\n",
    "\n",
    "        central_activation = self.relu(self.encoder_layers[-1](encoder_activations[-1]))\n",
    "\n",
    "        decoder_activations = [central_activation]\n",
    "\n",
    "        for idx, (decoder_layer, encoder_activation) in enumerate(zip(self.decoder_layers, encoder_activations[::-1])):\n",
    "            activation = decoder_activations[-1]\n",
    "            activation = nn.functional.layer_norm(activation, activation.shape[1:])\n",
    "            activation = nn.functional.interpolate(activation, size=encoder_activation.shape[2:])\n",
    "            activation = decoder_layer(activation)\n",
    "            activation = self.relu(activation) if idx != len(self.decoder_layers) - 1 else torch.sigmoid(activation)\n",
    "            decoder_activations.append(activation)\n",
    "\n",
    "        return decoder_activations[-1].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f9c83-070e-40be-80c7-4522b316046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DeepImagePrior.forward\" class=\"doc_header\"><code>DeepImagePrior.forward</code><a href=\"__main__.py#L32\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DeepImagePrior.forward</code>()\n",
       "\n",
       "The forward pass of the DIP with a fixed random noise input. Returns a `torch.Tensor` object."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DeepImagePrior.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36eedaa-d18e-454c-8fe5-bef3bad0cf74",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2a482-9e58-46ef-8c09-264fa5488a6c",
   "metadata": {},
   "source": [
    "[1] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. \"Deep image prior.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0f688-ef35-44df-bb63-779ff47494c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import hypothesis.strategies as st\n",
    "from hypothesis import given, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d937397-8d9f-41ca-bf74-38acd9f8b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "st_n_channels = st.integers(min_value=1, max_value=10)\n",
    "st_shape = st.tuples(st.integers(min_value=5, max_value=50),\n",
    "                     st.integers(min_value=5, max_value=50),\n",
    "                     st.integers(min_value=5, max_value=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7fb735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 421 ms, sys: 114 ms, total: 536 ms\n",
      "Wall time: 72.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#hide\n",
    "\n",
    "@given(n_channels=st_n_channels, shape=st_shape)\n",
    "@settings(max_examples=5, deadline=None)\n",
    "def test_shapes(n_channels, shape):\n",
    "        dip = DeepImagePrior(shape=shape, n_channels=n_channels)\n",
    "        θ = dip()\n",
    "        assert θ.shape == (n_channels, *shape)\n",
    "\n",
    "\n",
    "test_shapes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
