{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab885f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09460b32",
   "metadata": {},
   "source": [
    "# Convolutional block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589ee9f-324e-4d55-8267-8e71ac85e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines a convolutional block that can be used for the construction of convolutional neural networks (CNNs).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions:int, # The number of dimensions to consider. Possible options are 2 and 3.\n",
    "        in_channels:int, # The number of input channels.\n",
    "        out_channels:int, # The number of output channels.\n",
    "        normalization:str=None, # The type of normalization to use. Possible options include \"batch\", \"layer\" and \"instance\".\n",
    "        kernel_size:int=3, # The size of the convolutional kernel.\n",
    "        activation:str='ReLU', # The activation function that should be used.\n",
    "        preactivation:bool=False, # Whether to use preactivations.\n",
    "        use_padding:bool=True, # Whether to use padding.\n",
    "        padding_mode:str='zeros', # The type of padding to use.\n",
    "        dilation:bool=None, # The amount of dilation that should be used.\n",
    "        dropout:float=0, # The dropout rate.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dimensions = dimensions\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalization = normalization\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.preactivation = preactivation\n",
    "        self.use_padding = use_padding\n",
    "        self.padding_mode = padding_mode\n",
    "        self.dropout = dropout\n",
    "\n",
    "        if dilation is None:\n",
    "            self.dilation = 1\n",
    "\n",
    "        self._set_padding()\n",
    "\n",
    "        self._set_conv_layer()\n",
    "        self._set_norm_layer()\n",
    "        self._set_activation_layer()\n",
    "        self._set_dropout_layer()\n",
    "\n",
    "        self.block = self._create_sequential_block_from_layers()\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                x:torch.Tensor # The input to the convolutional block.\n",
    "               ):\n",
    "        \"\"\"\n",
    "        The forward pass of the convolutional block. Returns a `torch.Tensor`.\n",
    "        \"\"\"\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "    def _set_padding(self):\n",
    "        if self.use_padding:\n",
    "            total_padding = self.dilation * (self.kernel_size - 1)\n",
    "            self.padding = total_padding // 2\n",
    "        else:\n",
    "            self.padding = 0\n",
    "\n",
    "\n",
    "    def _set_conv_layer(self):\n",
    "        conv_class = getattr(nn, f'Conv{self.dimensions}d')\n",
    "        no_bias = not self.preactivation and (self.normalization is not None)\n",
    "\n",
    "        self.conv_layer = conv_class(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            padding_mode=self.padding_mode,\n",
    "            dilation=self.dilation,\n",
    "            bias=not no_bias\n",
    "        )\n",
    "\n",
    "\n",
    "    def _set_norm_layer(self):\n",
    "        if self.normalization is None:\n",
    "            self.norm_layer = None\n",
    "        else:\n",
    "            num_features = self.in_channels if self.preactivation else self.out_channels\n",
    "            norm_class = getattr(nn, f'{self.normalization.capitalize()}Norm{self.dimensions}d')\n",
    "            self.norm_layer = norm_class(num_features)\n",
    "\n",
    "\n",
    "    def _set_activation_layer(self):\n",
    "        if self.activation is None:\n",
    "            self.activation_layer = None\n",
    "        elif type(self.activation) == str:\n",
    "            self.activation_layer = getattr(nn, self.activation)()\n",
    "        else:\n",
    "            self.activation_layer = self.activation\n",
    "\n",
    "\n",
    "    def _set_dropout_layer(self):\n",
    "        if self.dropout == 0 or self.dropout is None:\n",
    "            self.dropout_layer = None\n",
    "        else:\n",
    "            dropout_class = getattr(nn, f'Dropout{self.dimensions}d')\n",
    "            self.dropout_layer = dropout_class(p=self.dropout)\n",
    "\n",
    "\n",
    "    def _create_sequential_block_from_layers(self):\n",
    "        block = nn.ModuleList()\n",
    "\n",
    "        if self.preactivation:\n",
    "            self._add_if_not_none(block, self.norm_layer)\n",
    "            self._add_if_not_none(block, self.activation_layer)\n",
    "            self._add_if_not_none(block, self.conv_layer)\n",
    "        else:\n",
    "            self._add_if_not_none(block, self.conv_layer)\n",
    "            self._add_if_not_none(block, self.norm_layer)\n",
    "            self._add_if_not_none(block, self.activation_layer)\n",
    "\n",
    "        self._add_if_not_none(block, self.dropout_layer)\n",
    "        return nn.Sequential(*block)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_if_not_none(module_list, module):\n",
    "        if module is not None:\n",
    "            module_list.append(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1785eb-8d9f-4a10-b623-4952dd341c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ConvolutionalBlock.forward\" class=\"doc_header\"><code>ConvolutionalBlock.forward</code><a href=\"__main__.py#L45\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ConvolutionalBlock.forward</code>(**`x`**:`Tensor`)\n",
       "\n",
       "The forward pass of the convolutional block. Returns a `torch.Tensor`.\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`x`**|`Tensor`||The input to the convolutional block.|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ConvolutionalBlock.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dab7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.38 ms, sys: 2.66 ms, total: 10 ms\n",
      "Wall time: 29.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#hide\n",
    "\n",
    "def test_that_we_can_instanciate():\n",
    "    topo_solver = ConvolutionalBlock(3, 1, 1)\n",
    "\n",
    "\n",
    "test_that_we_can_instanciate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
