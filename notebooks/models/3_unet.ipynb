{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ca076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union\n",
    "\n",
    "from dl4to.models import ConvolutionalBlock, Encoder, EncodingBlock, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feec32b-61b3-4e93-bcc4-fe938dbeaa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbd2c1-df04-42a1-b579-8f6b9bec589e",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd101875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNets are convolutional autoencoders that were developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg [1].\n",
    "    The network is based on a fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. \n",
    "    Our code based on `https://github.com/fepegar/unet/tree/master/unet`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels:int=7, # The number of input channels.\n",
    "        out_classes:int=1, # The number of output classes.\n",
    "        dimensions:int=3, # The number of dimensions to consider. Possible options are 2 and 3.\n",
    "        num_encoding_blocks:int=4, # The number of encoding blocks.\n",
    "        out_channels_first_layer:int=10, # The number of output channels after the first encoding step.\n",
    "        normalization:str='batch', # The type of normalization to use. Possible options include \"batch\", \"layer\" and \"instance\".\n",
    "        pooling_type:str='max', # The type of pooling to use.\n",
    "        upsampling_type:str='nearest', # The type of upsampling to use.\n",
    "        preactivation:bool=False, # Whether to use preactivations.\n",
    "        residual:bool=False, # Whether the encoder should be a residual network.\n",
    "        use_padding:bool=True, # Whether to use padding.\n",
    "        padding_mode:str='zeros', # The type of padding to use.\n",
    "        activation:str='ReLU', # The activation function that should be used.\n",
    "        classifier_activation:str='Sigmoid', # The activation function for the classifier at the end of the UNet.\n",
    "        initial_dilation:int=None, # The amount of dilation that should be used in the first encoding block.\n",
    "        dropout:float=0., # The dropout rate.\n",
    "        upsample_recover_orig_size:bool=True, # Whether the original input size of the encoder should be recovered with the decoder output.\n",
    "        pooling_kernel_size:Union[int,list]=[3, 3, 3], # The size of the pooling kernel.\n",
    "        use_classifier:bool=True, # Whether to use a classifier layer at the end of the network.\n",
    "        verbose:bool=True # Whether to print the user information on the neural network, for instance the number of parameters.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_classes = out_classes\n",
    "        self.dimensions = dimensions\n",
    "        self.num_encoding_blocks = num_encoding_blocks\n",
    "        self.out_channels_first_layer = out_channels_first_layer\n",
    "        self.normalization = normalization\n",
    "        self.pooling_type = pooling_type\n",
    "        self.upsampling_type = upsampling_type\n",
    "        self.preactivation = preactivation\n",
    "        self.residual = residual\n",
    "        self.use_padding = use_padding\n",
    "        self.padding_mode = padding_mode\n",
    "        self.activation = activation\n",
    "        self.classifier_activation = classifier_activation\n",
    "        self.initial_dilation = initial_dilation\n",
    "        self.dropout = dropout\n",
    "        self.upsample_recover_orig_size = upsample_recover_orig_size\n",
    "        self.pooling_kernel_size = pooling_kernel_size\n",
    "        self.use_classifier = use_classifier\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.depth = self.num_encoding_blocks - 1\n",
    "\n",
    "        if residual:\n",
    "            self.use_padding = True\n",
    "\n",
    "        if len(pooling_kernel_size) != dimensions:\n",
    "            raise ValueError('Length of pooling_kernel_size and dimension do not coincide!')\n",
    "\n",
    "\n",
    "        self.encoder = self._get_encoder()\n",
    "        self.bottom_block = self._get_bottom_block()\n",
    "        self.decoder = self._get_decoder()\n",
    "\n",
    "        if self.use_classifier:\n",
    "            self.classifier = self._get_classifier()\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "            print(f'Built model with {n_params} parameters.')\n",
    "\n",
    "\n",
    "    def _get_encoder(self):\n",
    "        encoder = Encoder(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels_first=self.out_channels_first_layer,\n",
    "            dimensions=self.dimensions,\n",
    "            pooling_type=self.pooling_type,\n",
    "            num_encoding_blocks=self.depth,\n",
    "            normalization=self.normalization,\n",
    "            preactivation=self.preactivation,\n",
    "            residual=self.residual,\n",
    "            use_padding=self.use_padding,\n",
    "            padding_mode=self.padding_mode,\n",
    "            activation=self.activation,\n",
    "            initial_dilation=self.initial_dilation,\n",
    "            dropout=self.dropout,\n",
    "            pooling_kernel_size=self.pooling_kernel_size\n",
    "        )\n",
    "        return encoder\n",
    "\n",
    "\n",
    "    def _get_bottom_block(self):\n",
    "        out_channels_first = self.encoder.out_channels\n",
    "\n",
    "        if self.dimensions == 2:\n",
    "            out_channels_first = 2 * out_channels_first\n",
    "\n",
    "        bottom_block = EncodingBlock(\n",
    "            in_channels=self.encoder.out_channels,\n",
    "            out_channels_first=out_channels_first,\n",
    "            dimensions=self.dimensions,\n",
    "            normalization=self.normalization,\n",
    "            pooling_type=None,\n",
    "            preactivation=self.preactivation,\n",
    "            residual=self.residual,\n",
    "            use_padding=self.use_padding,\n",
    "            padding_mode=self.padding_mode,\n",
    "            activation=self.activation,\n",
    "            dilation=self.encoder.dilation,\n",
    "            dropout=self.dropout,\n",
    "            pooling_kernel_size=self.pooling_kernel_size\n",
    "        )\n",
    "        return bottom_block\n",
    "\n",
    "\n",
    "    def _get_decoder(self):\n",
    "        power = self.depth\n",
    "\n",
    "        if self.dimensions == 2:\n",
    "            power = power - 1\n",
    "\n",
    "        decoder = Decoder(\n",
    "            in_channels_skip_connection=self.out_channels_first_layer * 2**power,\n",
    "            dimensions=self.dimensions,\n",
    "            upsampling_type=self.upsampling_type,\n",
    "            num_decoding_blocks=self.depth,\n",
    "            normalization=self.normalization,\n",
    "            preactivation=self.preactivation,\n",
    "            residual=self.residual,\n",
    "            use_padding=self.use_padding,\n",
    "            padding_mode=self.padding_mode,\n",
    "            activation=self.activation,\n",
    "            initial_dilation=self.encoder.dilation,\n",
    "            dropout=self.dropout,\n",
    "            upsample_recover_orig_size=self.upsample_recover_orig_size\n",
    "        )\n",
    "        return decoder\n",
    "\n",
    "\n",
    "    def _get_classifier(self):\n",
    "        in_channels = self.bottom_block.out_channels\n",
    "\n",
    "        if self.dimensions == 2:\n",
    "            in_channels = self.out_channels_first_layer\n",
    "        elif self.dimensions == 3:\n",
    "            in_channels = 2 * self.out_channels_first_layer\n",
    "\n",
    "        classifier = ConvolutionalBlock(\n",
    "            dimensions=self.dimensions,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=self.out_classes,\n",
    "            kernel_size=1,\n",
    "            activation=self.classifier_activation\n",
    "        )\n",
    "        return classifier\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                model_inputs:torch.Tensor # The input to the UNet.\n",
    "               ):\n",
    "        \"\"\"\n",
    "        The forward pass of the UNet. Returns a `torch.Tensor` object.\n",
    "        \"\"\"\n",
    "        skip_connections, encoding = self.encoder(model_inputs)\n",
    "        encoding = self.bottom_block(encoding)\n",
    "\n",
    "        output = self.decoder(skip_connections, encoding)\n",
    "\n",
    "        if self.use_classifier:\n",
    "            return self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9fd24-a4db-4d1f-9609-b7e1af172aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"UNet.forward\" class=\"doc_header\"><code>UNet.forward</code><a href=\"__main__.py#L162\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>UNet.forward</code>(**`model_inputs`**:`Tensor`)\n",
       "\n",
       "The forward pass of the UNet. Returns a `torch.Tensor` object.\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`model_inputs`**|`Tensor`||The input to the UNet.|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(UNet.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ce82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UNet3D(UNet):\n",
    "    \"\"\"\n",
    "    A 3d version of our UNet. UNets are convolutional autoencoders that were developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg [1].\n",
    "    The network is based on a fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. \n",
    "    Our code based on `https://github.com/fepegar/unet/tree/master/unet`.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **user_kwargs):\n",
    "        kwargs = {}\n",
    "        kwargs['dimensions'] = 3\n",
    "        kwargs.update(user_kwargs)\n",
    "        super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1dde85-a575-4c77-bac0-e96a82d9ea15",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3042416-3aa7-4bf9-bb4b-fb4ccc69c6e3",
   "metadata": {},
   "source": [
    "[1] Falk, Thorsten, et al. \"U-Net: deep learning for cell counting, detection, and morphometry.\" Nature methods 16.1 (2019): 67-70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab4534-5ee0-4955-9b9b-5d96cc2614b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import hypothesis.strategies as st\n",
    "from hypothesis import given, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810d875-979e-49dc-9524-bb3fb3f5a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "st_n_channels = st.integers(min_value=1, max_value=50)\n",
    "st_input_shape = st.tuples(\n",
    "    st.integers(min_value=6, max_value=50),\n",
    "    st.integers(min_value=6, max_value=50),\n",
    "    st.integers(min_value=6, max_value=50)\n",
    ")\n",
    "st_n_output_classes = st.integers(min_value=1, max_value=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490c3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 81 ms, total: 22.3 s\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#hide\n",
    "@given(\n",
    "    n_channels=st_n_channels,\n",
    "    input_shape=st_input_shape,\n",
    "    n_output_classes=st_n_output_classes\n",
    ")\n",
    "@settings(max_examples=5, deadline=None)\n",
    "def test_output_shapes_in_3d(n_channels, input_shape, n_output_classes):\n",
    "    model = UNet3D(\n",
    "        in_channels=n_channels,\n",
    "        out_classes=n_output_classes,\n",
    "        num_encoding_blocks=2,\n",
    "        verbose=False\n",
    "    ).eval()\n",
    "\n",
    "    x = torch.rand(1, n_channels, *input_shape)\n",
    "    assert model(x).shape ==  (1, n_output_classes, *input_shape)\n",
    "\n",
    "\n",
    "test_output_shapes_in_3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f10134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.1 s, sys: 65.8 ms, total: 22.1 s\n",
      "Wall time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#hide\n",
    "@given(input_shape=st_input_shape)\n",
    "@settings(max_examples=5, deadline=None)\n",
    "def test_output_shapes_in_3d(input_shape):\n",
    "    model = UNet3D(\n",
    "        num_encoding_blocks=2,\n",
    "        verbose=False\n",
    "    ).eval()\n",
    "\n",
    "    x = torch.rand(1, 7, *input_shape)\n",
    "    assert model(x).shape ==  (1, 1, *input_shape)\n",
    "\n",
    "\n",
    "test_output_shapes_in_3d()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
