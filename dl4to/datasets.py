# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/datasets/5_simp_dataset.ipynb (unless otherwise specified).

__all__ = ['CSVConverter', 'TopoDataset', 'CombinedTopoDataset', 'BasicDataset', 'CSVDataset', 'SELTODataset',
           'SIMPDataset']

# Internal Cell
import os
import torch
import numpy as np
import pandas as pd
import warnings
from tqdm.notebook import tqdm

from .problem import Problem
from .solution import Solution

# Cell
class CSVConverter():
    """
    The purpose of the CSVConverter class is to convert csv files into TO problems. For each problem there should be two csv files:
    one that contains all voxel-wise information (forces, design space, Dirichlet conditions and ground truth densities) and one "_info" file that contains all scalar information (Young's modulus E, Poisson's ratio ν, yield stress σ_ys and voxel size h).
    The names of the files should start with 0 and increase, so the files for the first sample are "0.csv" and "0_info.csv", the files for the second sample are "1.csv" and "1_info.csv" and so one. For more information on the exact formating we refer to
    the SELTO datasets [1].
    """
    def __init__(
        self,
        csv_dir_path:str, # The path to the directory that contains files named as "i.pt" and "i_info.pt" where i is an integer, starting at i=0 for the first sample.
        dtype:torch.dtype=torch.float32, # The datatype into which the values from the csv files are converted.
        verbose:bool=True, # Whether to give the user feedback on the progress.
        pde_solver:"pd4to.pde.PDESolver"=None, # The pde solver that is used to solve the PDE for linear elasticity. Only has an effect if either `solve_pde_for_trivial_solution=True` or `solve_pde_for_gt_solution=True`.
        solve_pde_for_trivial_solution:bool=False, # Whether to solve the PDE for each trivial solution and save the displacements in the solution object. These can later be accessed via `problem.trivial_solution.u`. This is useful if PDE preprocessing is used. Requires a pde solver.
        solve_pde_for_gt_solution:bool=False # Whether to solve the PDE for each ground truth and save the displacements in the solution object. These can later be accessed via `gt_solution.u`. Requires a pde solver.
    ):
        self._csv_dir_path = csv_dir_path
        self._dtype = dtype
        self.verbose = verbose
        self.solve_pde_for_trivial_solution = solve_pde_for_trivial_solution
        self.solve_pde_for_gt_solution = solve_pde_for_gt_solution
        self.pde_solver = pde_solver
        self.column_names = [
            'x', 'y', 'z',
            'design_space',
            'dirichlet_x', 'dirichlet_y', 'dirichlet_z',
            'force_x', 'force_y', 'force_z', 'density'
        ]


    @property
    def csv_dir_path(self):
        return self._csv_dir_path


    @property
    def dtype(self):
        return self._dtype


    @property
    def size(self):
        return self._size


    def __len__(self):
        """
        Returns the number of file pairs `(i.csv, i_info.csv)` that are contained in `self.csv_dir_path`.
        """
        csv_files = os.listdir(self.csv_dir_path)
        n_csv_files = 0

        for csv_file in csv_files:
            if not 'info' in csv_file:
                n_csv_files += 1
        return n_csv_files


    def __getitem__(self,
                    idx:int # The index for which `(problem, gt_solution)` should be returned.
                   ):
        """
        Returns the tuple `(problem, gt_solution)` for index `idx`.
        """
        data = self._get_data_array(idx)
        shape, voxels = self._get_shape_and_voxels(data)
        E, ν, σ_ys, h = self._get_data_info(shape, idx)
        F, Ω_dirichlet, Ω_design = self._get_forces_boundary_conditions_and_design_space(data, shape, voxels)
        θ = self._get_θ(data, shape, voxels)

        problem = Problem(
            E=E, ν=ν, σ_ys=σ_ys, h=h,
            Ω_dirichlet=Ω_dirichlet,
            Ω_design=Ω_design,
            F=F,
            pde_solver=self.pde_solver,
            name=f"problem_{idx}",
            dtype=self.dtype)

        if self.solve_pde_for_trivial_solution:
            _ = problem.trivial_solution.solve_pde()

        gt_solution = Solution(
            problem=problem,
            θ=θ
        )

        if self.solve_pde_for_gt_solution:
            _ = gt_solution.solve_pde(binary=True)

        return problem, gt_solution


    def _get_data_info(self, shape, i):
        file_path = f'{self.csv_dir_path}/{i}_info.csv'
        data_info_column_names = ['E', 'ν', 'σ_ys', 'h']
        data_info = pd.read_csv(file_path,  names=data_info_column_names)
        E = data_info['E'].item()
        ν = data_info['ν'].item()
        σ_ys = data_info['σ_ys'].item()
        h = data_info['h'].item()
        return E, ν, σ_ys, [h, h, h]


    def _get_data_array(self, i):
        data = pd.read_csv(f'{self.csv_dir_path}/{i}.csv', names=self.column_names)
        return data


    def _get_shape_and_voxels(self, data):
        shape = data[['x', 'y', 'z']].iloc[-1].values.astype(int) + 1
        vox_x = data['x'].values
        vox_y = data['y'].values
        vox_z = data['z'].values
        voxels = [vox_x, vox_y, vox_z]
        return shape, voxels


    def _get_forces_boundary_conditions_and_design_space(self, data, shape, voxels):
        F = torch.zeros(3, *shape, dtype=self.dtype)
        F[0, voxels[0], voxels[1], voxels[2]] = torch.tensor(data['force_x'].values, dtype=self.dtype)
        F[1, voxels[0], voxels[1], voxels[2]] = torch.tensor(data['force_y'].values, dtype=self.dtype)
        F[2, voxels[0], voxels[1], voxels[2]] = torch.tensor(data['force_z'].values, dtype=self.dtype)

        Ω_dirichlet = torch.zeros(3, *shape, dtype=self.dtype)
        Ω_dirichlet[0, voxels[0], voxels[1], voxels[2]] = torch.tensor(data['dirichlet_x'].values, dtype=self.dtype)
        Ω_dirichlet[1, voxels[0], voxels[1], voxels[2]] = torch.tensor(data['dirichlet_y'].values, dtype=self.dtype)
        Ω_dirichlet[2, voxels[0], voxels[1], voxels[2]] = torch.tensor(data['dirichlet_z'].values, dtype=self.dtype)

        Ω_design = torch.zeros(1, *shape, dtype=int)
        Ω_design[:, voxels[0], voxels[1], voxels[2]] = torch.from_numpy(data['design_space'].values.astype(int))
        return F, Ω_dirichlet, Ω_design


    def _get_θ(self, data, shape, voxels):
        θ = torch.zeros(1, *shape, dtype=self.dtype)
        θ[:, voxels[0], voxels[1], voxels[2]] = torch.tensor(data['density'].values, dtype=self.dtype)
        return θ


    def __call__(self,
                 pt_dir_path:str=None # The path where the `.pt` files should be saved.
                ):
        """
        Converts all `(i.csv, i_info.csv)` pairs in `csv_dir_path` and saves them as `.pt` files in the directory `pt_dir_path`.
        """
        if pt_dir_path is None:
            pass
        else:
            if not os.path.exists(pt_dir_path):
                os.makedirs(pt_dir_path)

        iters = range(len(self))
        if self.verbose:
            print("Generating dataset...")
            iters = tqdm(iters)

        for i in iters:
            try:
                sample = self[i]
                if pt_dir_path is not None:
                    torch.save(sample, f"{pt_dir_path}/{i}.pt")
            except FileNotFoundError:
                if self.verbose:
                    print(f"Could not generate file {i}.")

        if self.verbose:
            print("done!")
            if pt_dir_path is not None:
                print(f"pt dataset generated in directory `{pt_dir_path}`.")

# Internal Cell
import os
import torch
from torch.utils.data import Dataset
from torch.utils.data import random_split
from tqdm.notebook import tqdm
import warnings
import random
from typing import Union

# Cell
class TopoDataset(Dataset):
    """
    A class for the generation of datasets. TopoDataset inherits from `torch.utils.data.Dataset`, so all functionalities from PyTorch are also available here.
    """
    def __init__(self,
                 dataset:list=[], # A list containing either only problems or tuples `(problem, gt_solution)` of problems and corresponding ground truth solutions. By default, `dataset=[]`, so the dataset is empty. However, it can still be changed later via `TopoDataset.dataset=...`.
                 name:str=None, # The name of the dataset.
                 verbose:bool=True # Whether to give the user feedback on the progress.
                ):
        self.dataset = dataset
        self.name = name
        self.verbose = verbose


    @property
    def dataset(self):
        return self._dataset


    @dataset.setter
    def dataset(self, dataset_):
        self._dataset = dataset_
        self._size = len(dataset_)


    @property
    def size(self):
        return self._size


    def __len__(self):
        """
        Returns the size of `self.dataset`.
        """
        return len(self.dataset)


    def __getitem__(self,
                    idx:int # The index for which `(problem, gt_solution)` should be returned.
                   ):
        """
        Returns the tuple `(problem, gt_solution)` for index `idx`.
        """
        if idx >= len(self):
            raise IndexError(f"Could not find dataset entry with index {idx}.")
        return self.dataset[idx]


    def _build_empty_topo_dataset_with_same_attributes(self):
        topo_dataset = TopoDataset(verbose=False)
        topo_dataset.name = self.name
        return topo_dataset


    def get_samples(self,
                    n:int=-1, # The number of samples that should be returned. The default choice `n=-1` returns all samples from the dataset.
                    shuffle:bool=True, # Whether to take the samples from a shuffled dataset. If `False`, then the first samples from the dataset are taken.
                    seed:int=42 # The random seed for the shuffling
                   ):
        """
        Returns a tuple of lists `(problems, gt_solutions)` of length `n`.
        """
        if n == -1:
            n = len(self)
        else:
            n = min(len(self), n)

        if shuffle:
            random.seed(seed)
            samples = random.sample(self.dataset, n)
        else:
            samples = self.dataset[:n]

        return tuple(zip(*samples[:]))


    def get_problems(self,
                    n:int=-1, # The number of problems that should be returned. The default choice `n=-1` returns all problems from the dataset.
                    shuffle:bool=True, # Whether to take the problems from a shuffled dataset. If `False`, then the first problems from the dataset are taken.
                    seed:int=42 # The random seed for the shuffling
                   ):
        """
        Returns a list of length `n` which contains problems from the dataset.
        """
        samples = self.get_samples(n=n, shuffle=shuffle, seed=seed)
        if len(samples) > 0:
            return samples[0]


    def get_gt_solutions(self,
                    n:int=-1, # The number of ground truth solutions that should be returned. The default choice `n=-1` returns all solutions from the dataset.
                    shuffle:bool=True, # Whether to take the solutions from a shuffled dataset. If `False`, then the first solutions from the dataset are taken.
                    seed:int=42 # The random seed for the shuffling
                        ):
        """
        Returns a list of length `n` which contains ground truth solutions from the dataset.
        """
        samples = self.get_samples(n=n, shuffle=shuffle, seed=seed)
        if len(samples) > 1:
            return samples[1]


    def get_subset(self,
                   size:int, # The size of the returned topo dataset.
                   shuffle=True, # Whether to take the samples from a shuffled dataset. If `False`, then the first samples from the dataset are taken.
                   seed=42, # The random seed for the shuffling
                   invert_order=False # Whether the last samples should be taken (instead of the first samples). Only has an effect if `shuffle=False`.
                  ):
        """
        Returns a new `dl4to.dataset.TopoDataset` object with a subset of `size` samples from the original dataset.
        """
        if shuffle:
            random.seed(seed)
            dataset = random.sample(self.dataset, len(self.dataset))
        else:
            dataset = self.dataset

        topo_dataset = self._build_empty_topo_dataset_with_same_attributes()
        if invert_order:
            topo_dataset.dataset = dataset[-size:]
        else:
            topo_dataset.dataset = dataset[:size]
        topo_dataset._size = len(topo_dataset.dataset)
        return topo_dataset


    def info(self):
        """
        Prints basic information concerning the dataset.
        """
        print(f"This TopoDataset is called {self.name} and contains {len(self)} samples.")


    def __add__(self,
                dataset:Union["dl4to.dataset.TopoDataset",list] # The dataset that is added to this one. If `dataset` is a list, then the samples in the list are added to the current dataset.
               ):
        """
        Adding up two datasets results in a new dataset object that contains the samples from both original datasets.
        """
        if issubclass(type(dataset), TopoDataset):
            return CombinedTopoDataset(self, dataset)
        if type(dataset) == list:
            self.dataset += dataset
            return self
        raise AttributeError("dataset must be either a list or a TopoDataset object.")

# Cell
class CombinedTopoDataset(TopoDataset):
    """
    A class that results from the summation of two topo datasets.
    """
    def __init__(self,
                 dataset1:"dl4to.dataset.TopoDataset", # The first dataset of the summation.
                 dataset2:"dl4to.dataset.TopoDataset", # The second dataset of the summation.
                ):
        self._size = dataset1.size + dataset2.size
        self.name = f'{dataset1.name}_plus_{dataset2.name}'
        self.verbose = dataset1.verbose

        self.topo_dataset1 = dataset1
        self.topo_dataset2 = dataset2

        self.dataset1_ratio = len(self.topo_dataset1) / len(self)

        if self.verbose != dataset2.verbose:
            self.verbose = True
            warnings.warn(f"`verbose` attribute of the two datasets does not coincide. Automatically setting `verbose=True`.")
        if self.verbose:
            from_dataset1 = len(self.topo_dataset1)*[1] + len(self.topo_dataset2)*[0]
            print(f"Created combined dataset. {sum(from_dataset1)} of the samples (={100*self.dataset1_ratio:.2f}%) are from the first passed dataset (total: {len(self)} samples).")


    @property
    def dataset(self):
        return self.topo_dataset1.dataset + self.topo_dataset2.dataset


    def get_subset(self,
                   size:int, # The size of the returned topo dataset.
                   shuffle:bool=True, # Whether the dataset should be shuffled. If `False`, then the first samples from both datasets are taken.
                   seed:int=42, # The random seed for the shuffling.
                   balanced:bool=True # Whether the ratio between `dataset1` and `dataset2` should be maintained in the subset.
                  ):
        """
        Returns an instance of `dl4to.dataset.TopoDataset` with a subset of `size` samples from the original dataset.
        """
        if not balanced:
            return super().get_subset(size=size, shuffle=shuffle, seed=seed)

        size_dataset1 = round(size * self.dataset1_ratio)
        size_dataset2 = size - size_dataset1
        if shuffle:
            random.seed(seed)
            dataset1 = random.sample(self.topo_dataset1.dataset, len(size_dataset1))
            random.seed(seed)
            dataset2 = random.sample(self.topo_dataset2.dataset, len(size_dataset2))
        else:
            dataset1 = self.topo_dataset1.dataset[:len(size_dataset1)]
            dataset2 = self.topo_dataset2.dataset[:len(size_dataset2)]
        topo_dataset = self._build_empty_topo_dataset_with_same_attributes()
        topo_dataset.dataset = dataset1 + dataset2
        return topo_dataset

# Internal Cell
import torch

from .problem import Problem
from .datasets import TopoDataset

# Internal Cell
class CreateBasicProblems:
    @staticmethod
    def _get_blanks(size, resolution, dtype):
        shape = torch.round(resolution * size).type(torch.int)
        h = size / shape
        F = torch.zeros(3, *shape, dtype=dtype, requires_grad=False)
        Ω_dirichlet = torch.zeros(3, *shape, dtype=torch.bool, requires_grad=False)
        Ω_design = - torch.ones(1, *shape, dtype=int, requires_grad=False)
        return shape, h, F, Ω_dirichlet, Ω_design


    @staticmethod
    def _transform_force_per_area_to_force_per_volume(F, h):
        F[0, ...] /= h[0]
        F[1, ...] /= h[1]
        F[2, ...] /= h[2]
        return F


    @staticmethod
    def _transform_point_force_to_force_per_volume(F, h):
        return F / (h[0] * h[1] * h[2])


    @staticmethod
    def _get_padded_tensor(tensor, padding_depth=2):
        p_d = int(padding_depth)
        if p_d == 0:
            return tensor

        shape = tensor.shape
        assert len(shape) == 4
        padded_tensor = torch.zeros(shape[0], shape[1]+2*p_d, shape[2]+2*p_d, shape[3]+2*p_d, dtype=tensor.dtype)
        padded_tensor[:, p_d:-p_d, p_d:-p_d, p_d:-p_d] = tensor
        return padded_tensor


    @staticmethod
    def _tensile_rod(resolution=20, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'tensile_rod'
        size = torch.tensor([.1, .1, 1.])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        F[-1, :, :, 0] = force_per_area
        Ω_dirichlet[:, :, :, -1] = 1
        F = CreateBasicProblems._transform_force_per_area_to_force_per_volume(F, h)
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _offset_tensile_rod(resolution=20, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'offset_tensile_rod'
        size = torch.tensor([.1, .1, 1.])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        F[-1, :, :, 4] = force_per_area
        Ω_dirichlet[:, :, :, -4] = 1
        F = CreateBasicProblems._transform_force_per_area_to_force_per_volume(F, h)
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _inverse_tensile_rod(resolution=20, force_per_area=1.5e5, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'inverse_tensile_rod'
        size = torch.tensor([.1, .1, 1.])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        F[-1, :, :, -1] = force_per_area
        Ω_dirichlet[:, :, :, 0] = 1
        F = CreateBasicProblems._transform_force_per_area_to_force_per_volume(F, h)
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _ledge(resolution=20, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'ledge'
        size = torch.tensor([1., .1, .2])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        F[-1, :, :, -1] = force_per_area
        Ω_dirichlet[:, 0, :, :] = 1
        F = CreateBasicProblems._transform_force_per_area_to_force_per_volume(F, h)
        Ω_design[:,:,:,-2] = 1.
        Ω_design[:,1,:,:] = 1.
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _cube_ledge(resolution=20, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'cube_ledge'
        size = torch.tensor([.1, .1, 1.])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        F[-1, :, :, -1] = force_per_area
        Ω_dirichlet[:, 0, :, :] = 1
        F = CreateBasicProblems._transform_force_per_area_to_force_per_volume(F, h)
        Ω_design[:,:,:,-2] = 1.
        Ω_design[:,1,:,:] = 1.
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _fork(resolution=20, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'fork'
        size = torch.tensor([1., .1, .2])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        F[-1, :, :, -1] = force_per_area
        Ω_dirichlet[:, 0:2, :, :] = 1
        F = CreateBasicProblems._transform_force_per_area_to_force_per_volume(F, h)
        Ω_design[:,:,:,-2] = 1.
        Ω_design[:,:,:,:2] = 1.
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _inverse_fork(resolution=20, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'fork'
        size = torch.tensor([1., .1, .2])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        F[-1, :, :, 0] = -force_per_area
        Ω_dirichlet[:, 0:2, :, :] = 1
        F = CreateBasicProblems._transform_force_per_area_to_force_per_volume(F, h)
        Ω_design[:,:,:, 1] = 1.
        Ω_design[:,:,:,-2:] = 1.
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _cantilever(resolution=20, point_force=-1e7, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'cantilever'
        size = torch.tensor([1., .1, .3])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        center_y = int(shape[1]/2)
        F[-1, -1, center_y, 0] = point_force
        Ω_dirichlet[:, 0, :, :] = 1
        F = CreateBasicProblems._transform_point_force_to_force_per_volume(F, h)
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _supported_ledge(resolution=20, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'ledge'
        size = torch.tensor([1., .1, .2])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        F[-1, :, :, -1] = force_per_area
        Ω_dirichlet[:, 0, :, :] = 1
        F = CreateBasicProblems._transform_force_per_area_to_force_per_volume(F, h)
        Ω_design[:,:,:,-2] = 1.
        Ω_design[:,1,:,:] = 1.
        Ω_design[:,-3:-1,:,:] = 1.
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _supported_unconnected_ledge(resolution=20, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'ledge'
        size = torch.tensor([1., .1, .2])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        F[-1, :, :, -1] = force_per_area
        Ω_dirichlet[:, 0, :, :] = 1
        F = CreateBasicProblems._transform_force_per_area_to_force_per_volume(F, h)
        Ω_design[:,:,:,-2] = 1.
        Ω_design[:,1,:,:] = 1.
        Ω_design[:,-3:-1,:,1:] = 1.
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)


    @staticmethod
    def _wheel(resolution=20, point_force=-2.5e6, E=7e10, ν=.3, σ_ys=4.5e8, dtype=torch.float32):
        name = 'wheel'
        size = torch.tensor([1., .4, .4])
        shape, h, F, Ω_dirichlet, Ω_design = CreateBasicProblems._get_blanks(size, resolution, dtype)
        center = shape / 2
        center0, center1 = int(center[0]), int(center[1])
        F[-1, center0-1:center0+1, center1-1:center1+1, 0] = point_force/4
        F = CreateBasicProblems._transform_point_force_to_force_per_volume(F, h)
        Ω_dirichlet[:, 0:2, :, -1] = 1
        Ω_dirichlet[:, -3:-1, :, -1] = 1
        Ω_design[0 != (torch.sum(Ω_dirichlet + (F != 0), dim=0, keepdim=True))] = 1
        return Problem(E=E, ν=ν, σ_ys=σ_ys, h=h, Ω_dirichlet=Ω_dirichlet,
                       Ω_design=Ω_design, F=F, name=name, dtype=dtype)

# Cell
class BasicDataset(TopoDataset):
    """
    A dataset that contains several basic problems that we found were common problems in the literature. The dataset itself contains no problems directly, but they can be individually accessed,
    via their respecive function calls, e.g., `dataset = BasicDataset().ledge()`. The dataset contains no ground truth solutions.
    """
    def __init__(self,
                 resolution:int=20, # The number of voxels of the dimension with the most voxels.
                 verbose:bool=True, # Whether to give the user feedback on the progress.
                 dtype:torch.dtype=torch.float32 # The datatype of the tensors that define the problem.
                ):
        self.resolution = resolution
        self._dtype = dtype
        super().__init__(verbose=verbose)


    @property
    def dtype(self):
        return self._dtype


    def tensile_rod(self, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._tensile_rod(resolution=self.resolution, force_per_area=force_per_area, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def offset_tensile_rod(self, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._offset_tensile_rod(resolution=self.resolution, force_per_area=force_per_area, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def inverse_tensile_rod(self, force_per_area=1.5e5, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._inverse_tensile_rod(resolution=self.resolution, force_per_area=force_per_area, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def ledge(self, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._ledge(resolution=self.resolution, force_per_area=force_per_area, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def cube_ledge(self, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._cube_ledge(resolution=self.resolution, force_per_area=force_per_area, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def fork(self, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._fork(resolution=self.resolution, force_per_area=force_per_area, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def inverse_fork(self, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._inverse_fork(resolution=self.resolution, force_per_area=force_per_area, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def cantilever(self, point_force=-1e7, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._cantilever(resolution=self.resolution, point_force=point_force, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def supported_ledge(self, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._supported_ledge(resolution=self.resolution, force_per_area=force_per_area, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def supported_unconnected_ledge(self, force_per_area=-1.5e5, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._supported_unconnected_ledge(resolution=self.resolution, force_per_area=force_per_area, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)


    def wheel(self, point_force=-2.5e6, E=7e10, ν=.3, σ_ys=4.5e8):
        return CreateBasicProblems()._wheel(resolution=self.resolution, point_force=point_force, E=E, ν=ν, σ_ys=σ_ys, dtype=self.dtype)

# Cell
import os
import shutil
import requests
import tarfile
import torch
from .datasets import TopoDataset, CSVConverter

# Cell

class CSVDataset(TopoDataset):
    """
    A class for downloading, generating and importing datasets from CSV files. An inheriting class needs to override the method `_get_gz_file_paths_dict` for correct paths to the `.tar.gz` files that contain the `csv` files (see e.g. SELTO datasets).
    """
    def __init__(
        self,
        root:str, # The root directory in which the datasets should be downloaded, generated and accessed.
        name:str, # The name of the dataset that should be downloaded.
        train:bool=True, # Whether the training or validation dataset should be generated.
        size:int=-1, # The size of the dataset. If `size=-1`, then the whole dataset is imported. Useful if only subsets of the original dataset are needed.
        download:bool=True, # Whether the dataset should be downloaded, if needed.
        verbose:bool=True, # Whether to give the user feedback on the progress.
        dtype:torch.dtype=torch.float32, # The datatype into which the values from the csv files are converted.
        pde_solver:"dl4to.pde.PDESolver"=None, # The PDE solver that is used to solve the PDE for linear elasticity. Only has an effect if either `solve_pde_for_trivial_solution=True` or `solve_pde_for_gt_solution=True`.
        solve_pde_for_trivial_solution:bool=False, # Whether to solve the PDE for each trivial solution and save the displacements in the solution object. These can later be accessed via `problem.trivial_solution.u`. This is useful if PDE preprocessing is used. Requires a PDE solver.
        solve_pde_for_gt_solution:bool=False # Whether to solve the PDE for each ground truth and save the displacements in the solution object. These can later be accessed via `gt_solution.u`. Requires a PDE solver.
    ):

        dataset_name = self._get_dataset_name(name, train)
        self._dtype = dtype
        super().__init__(
            name=dataset_name,
            verbose=verbose
        )
        self._size = size
        self._pt_dir_path = self._get_pt_dir_path(train, root, name)
        self._create_dirs(root, name)
        self.pt_file_paths = self._get_pt_file_paths()

        if len(self.pt_file_paths) == 0:
            self._generate_dataset(
                dataset_name=dataset_name,
                download=download,
                dtype=dtype,
                verbose=verbose,
                pde_solver=pde_solver,
                solve_pde_for_trivial_solution=solve_pde_for_trivial_solution,
                solve_pde_for_gt_solution=solve_pde_for_gt_solution
            )
            self.pt_file_paths = self._get_pt_file_paths()

        self._load_dataset()


    @property
    def pt_dir_path(self):
        return self._pt_dir_path


    @property
    def dtype(self):
        return self._dtype


    def _generate_dataset(self, dataset_name, download, dtype, verbose, pde_solver,
                          solve_pde_for_trivial_solution, solve_pde_for_gt_solution):
        gz_file_paths = [f'{self.pt_dir_path}/{file_name}' for file_name in os.listdir(self.pt_dir_path) if file_name[-2:] == 'gz']

        if len(gz_file_paths) == 0:
            if download:
                gz_file_path = self._download_gz_file(dataset_name)
            else:
                raise AttributeError('Dataset cannot be constructed with `download=False`.')
        else:
            if len(gz_file_paths) != 1:
                raise AttributeError('Directory contains more than one `.gz` file.')
            gz_file_path = gz_file_paths[0]

        csv_dir_path = self._extract_gz_file(gz_file_path, dataset_name)

        csv_converter = CSVConverter(
            csv_dir_path=csv_dir_path,
            dtype=dtype,
            verbose=verbose,
            pde_solver=pde_solver,
            solve_pde_for_trivial_solution=solve_pde_for_trivial_solution,
            solve_pde_for_gt_solution=solve_pde_for_gt_solution
        )

        csv_converter(self.pt_dir_path)
        shutil.rmtree(csv_dir_path)


    def _get_pt_file_paths(self):
        file_names = os.listdir(self.pt_dir_path)
        pt_file_paths = [f"{self.pt_dir_path}/{name}" for name in file_names if name[-2:] == 'pt']

        if self.size == -1 or self.size == 0:
            self._size = len(pt_file_paths)

        pt_file_paths = pt_file_paths[:self.size]

        if self.verbose:
            print(f"Found {len(pt_file_paths)} files.")

        return pt_file_paths


    def _load_dataset(self):
        self.dataset = []

        if self.verbose:
            print('importing dataset...')
            pt_file_paths = tqdm(self.pt_file_paths)

        for pt_file_path in pt_file_paths:
            self.dataset.append(torch.load(pt_file_path))

        if self.verbose:
            print('done!')


    def _get_pt_dir_path(self, train, root, name):
        if train:
            return f'{root}/{name}/train'
        return f'{root}/{name}/test'


    def _get_dataset_name(self, name, train):
        if train:
            return f'{name}_train'
        return f'{name}_test'


    def _create_dirs(self, root, name):
        if not os.path.exists(root):
            os.makedirs(root)

        if not os.path.exists(f'{root}/{name}'):
            os.makedirs(f'{root}/{name}')

        if not os.path.exists(self.pt_dir_path):
            os.makedirs(self.pt_dir_path)


    def _get_gz_file_paths_dict(self):
        raise NotImplementedError("Must be overridden.")


    def _download_gz_file(self, dataset_name):
        gz_file_path = f'{self.pt_dir_path}/{dataset_name}.tar.gz'
        resources = self._get_gz_file_paths_dict()
        url = resources[dataset_name]

        if self.verbose:
            print(f"Downloading {dataset_name}...")

        with open(gz_file_path, 'wb') as f:
            r = requests.get(url)
            f.write(r.content)

        if self.verbose:
            print("Done!")
        return gz_file_path


    def _extract_gz_file(self, gz_file_path, dataset_name):
        tar = tarfile.open(gz_file_path, 'r:gz')

        csv_dir_path = f'{self.pt_dir_path}/csv'

        if os.path.exists(csv_dir_path):
            shutil.rmtree(csv_dir_path)

        os.mkdir(csv_dir_path)

        if self.verbose:
            print(f"Extracting {dataset_name}...")

        tar.extractall(path=csv_dir_path)
        tar.close()

        if self.verbose:
            print("Done!")
        return csv_dir_path

# Cell
import os
import shutil
import requests
import tarfile
import torch
from .datasets import CSVDataset

# Cell
class SELTODataset(CSVDataset):
    """
    A class for downloading, generating and importing the SELTO datasets [1].
    """
    def __init__(
        self,
        root:str, # The root directory in which the datasets should be downloaded, generated and accessed.
        name:str, # The name of the dataset that should be downloaded.
        train:bool=True, # Whether the training or validation dataset should be generated.
        size:int=-1, # The size of the dataset. If `size=-1`, then the whole dataset is imported. Useful if only subsets of the original dataset are needed.
        download:bool=True, # Whether the dataset should be downloaded, if needed.
        verbose:bool=True, # Whether to give the user feedback on the progress.
        dtype:torch.dtype=torch.float32, # The datatype into which the values from the csv files are converted.
        pde_solver:"dl4to.pde.PDESolver"=None, # The PDE solver that is used to solve the PDE for linear elasticity. Only has an effect if either `solve_pde_for_trivial_solution=True` or `solve_pde_for_gt_solution=True`.
        solve_pde_for_trivial_solution:bool=False, # Whether to solve the PDE for each trivial solution and save the displacements in the solution object. These can later be accessed via `problem.trivial_solution.u`. This is useful if PDE preprocessing is used. Requires a PDE solver.
        solve_pde_for_gt_solution:bool=False # Whether to solve the PDE for each ground truth and save the displacements in the solution object. These can later be accessed via `gt_solution.u`. Requires a PDE solver.
    ):
        super().__init__(root=root,
                         name=name,
                         train=train,
                         size=size,
                         download=True,
                         verbose=verbose,
                         dtype=dtype,
                         pde_solver=pde_solver,
                         solve_pde_for_trivial_solution=solve_pde_for_trivial_solution,
                         solve_pde_for_gt_solution=solve_pde_for_gt_solution)


    def _get_gz_file_paths_dict(self):
        return {
            'disc_simple_train': 'https://zenodo.org/record/7781392/files/disc_simple_train.tar.gz?download=1',
            'disc_simple_test': 'https://zenodo.org/record/7781392/files/disc_simple_test.tar.gz?download=1',
            'disc_complex_train': 'https://zenodo.org/record/7781392/files/disc_complex_train.tar.gz?download=1',
            'disc_complex_test': 'https://zenodo.org/record/7781392/files/disc_complex_test.tar.gz?download=1',
            'sphere_simple_train': 'https://zenodo.org/record/7781392/files/sphere_simple_train.tar.gz?download=1',
            'sphere_simple_test': 'https://zenodo.org/record/7781392/files/sphere_simple_test.tar.gz?download=1',
            'sphere_complex_train': 'https://zenodo.org/record/7781392/files/sphere_complex_train.tar.gz?download=1',
            'sphere_complex_test': 'https://zenodo.org/record/7781392/files/sphere_complex_test.tar.gz?download=1'
        }

# Internal Cell
import torch
import numpy as np
from copy import deepcopy
from collections import defaultdict

from .datasets import TopoDataset

# Cell

class SIMPDataset(TopoDataset):
    def __init__(self, problems, simp, name=None, verbose=True):
        self.problems = problems
        self.simp = simp
        assert self.simp.return_intermediate_solutions == True
        dataset = self._generate_dataset()
        super().__init__(dataset=dataset,
                         name=name,
                         verbose=verbose)


    def _generate_dataset(self):
        self.list_of_problems_and_orig_solution_indices = defaultdict(list)
        dataset = []
        i_problem = 0
        for problem in self.problems:
            dataset.append((problem,  0.*problem.trivial_solution))
            dataset.append((problem, 0.5*problem.trivial_solution))
            solutions = self.simp(problems_or_solutions=problem)
            for solution in solutions:
                dataset.append((problem, solution.detach()))
                self.list_of_problems_and_orig_solution_indices[i_problem].append(len(dataset)-1)
            i_problem += 1
        return dataset


    def augment(self, pde_solver, max_augmentation_per_problem=5, threshold=1e-3):
        problems_ = deepcopy(self.problems)
        for i_problem, problem in enumerate(problems_):
            old_pde_solver = self.problems[i_problem].pde_solver
            problem.pde_solver = pde_solver
            augmentation_counter = 0
            simp_correct_counter = 0
            simp_deviations = []
            first_error = 0
            solutions = self.simp(problems_or_solutions=problem)
            solutions_to_add_to_dataset = []
            for i_solution, solution in enumerate(solutions):
                if augmentation_counter < max_augmentation_per_problem:
                    solution._θ = solution._θ.detach()
                    orig_simp_solution = self.dataset[self.list_of_problems_and_orig_solution_indices[i_problem][i_solution]][1]
                    simp_deviation = (solution._θ[problem.Ω_design==-1] - orig_simp_solution._θ[problem.Ω_design==-1]).abs().mean()
                    simp_deviations.append(simp_deviation.item())
                    if simp_deviation > threshold:
                        if augmentation_counter == 0:
                            first_error = simp_deviation
                        solutions_to_add_to_dataset.append(solution)
                        augmentation_counter += 1
                    else:
                        simp_correct_counter += 1
                else:
                    break
            problem.pde_solver = old_pde_solver
            for solution in solutions_to_add_to_dataset:
                solution.pde_solver = old_pde_solver
                solution.problem.pde_solver = old_pde_solver
                solution.u_current_θ = None
                self.dataset.append((solution.problem, solution))
            self._size += augmentation_counter
            print(f"Problem {i_problem}: {simp_correct_counter} correct SIMP iterations (next iteration had an error of {first_error:.3f}).")
            print(simp_deviations)
            print(f"Augmented dataset by {augmentation_counter} samples.")